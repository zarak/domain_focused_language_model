{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the matrix of prefix - word frequencies.\n",
    "Use the ngrams function from `nltk.utils` to generate all n-grams from the corpus\n",
    "Set the following `left_pad_symbol = <s>` and `right_pad_symbol = </s>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_pad_symbol = \"<s>\"\n",
    "right_pad_symbol = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 'e'), ('e', 's'), ('s', 't')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams('test', 2, left_pad_symbol=left_pad_symbol, right_pad_symbol=right_pad_symbol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens: \n",
    "# - unk represents absent tokens, \n",
    "# - eos is a special token after the end of sequence\n",
    "\n",
    "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
    "\n",
    "def count_ngrams(lines, n):\n",
    "    \"\"\"\n",
    "    Count how many times each word occured after (n - 1) previous words\n",
    "    :param lines: an iterable of strings with space-separated tokens\n",
    "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
    "\n",
    "    When building counts, please consider the following two edge cases\n",
    "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
    "      empty prefix: \"\" -> (UNK, UNK)\n",
    "      short prefix: \"the\" -> (UNK, the)\n",
    "      long prefix: \"the new approach\" -> (new, approach)\n",
    "    - you should add a special token, EOS, at the end of each sequence\n",
    "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
    "      count the probability of this token just like all others.\n",
    "    \"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
    "    for line in lines:\n",
    "        for words in ngrams(line.split(), n,\n",
    "                            pad_left=True,\n",
    "                            pad_right=True,\n",
    "                            left_pad_symbol=left_pad_symbol,\n",
    "                            right_pad_symbol=right_pad_symbol):\n",
    "            prefix, target = words[:-1], words[-1]\n",
    "            counts[prefix][target] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count_ngrams(['test this planet hurrah test this planet', 'another test this planet', 'test this spoon'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs[('another', 'test')]['this']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a text generation function:  \n",
    "- takes a bigram as input and generates the next token\n",
    "- iteratively slide the prefix over the generated text so that the new prefix includes the most recent token; generates the next token\n",
    "- to generate each next token, sample the list of words associated with the prefix using the probability distribution of the prefix.\n",
    "- stop the text generation when a certain number of words have been generated or the latest token is a `</s>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(v for _, v in freqs[('test', 'this')].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram = ('test', 'this')\n",
    "total_counts = sum(count for count in freqs[bigram].values())\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'this', 'spoon']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(freqs):\n",
    "    # Let's transform the counts to probabilities\n",
    "    for prefix in freqs:\n",
    "        total_count = float(sum(freqs[prefix].values()))\n",
    "        for w3 in freqs[prefix]:\n",
    "            freqs[prefix][w3] /= total_count\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed, model, max_length=10):\n",
    "    assert len(seed) < max_length, \"Max length must be greater than the length of the seed\"\n",
    "    sentence_finished = False\n",
    "\n",
    "    while (not sentence_finished) and len(seed) <= max_length:\n",
    "        probs = list(model[tuple(seed[-2:])].values())\n",
    "        words = list(model[tuple(seed[-2:])].keys())\n",
    "        seed.append(np.random.choice(words, p=probs))\n",
    "        if seed[-2:] == ['</s>', '</s>']:\n",
    "            sentence_finished = True\n",
    "    return ' '.join([t for t in seed if t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test this planet hurrah test this planet </s> </s>'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"test\", \"this\"]\n",
    "generate_text(text, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that can estimate the probability of a sentence and use it to select the most probable sentence out of several candidate sentences\n",
    "- Split the sentence into trigrams and use the chain rule to calculate the probability of the sentence as a product of the bigrams - tokens probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
