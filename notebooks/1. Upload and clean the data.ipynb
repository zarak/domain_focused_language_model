{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OeIHZdTXg03B"
   },
   "source": [
    "# Task 1: Upload and clean the data\n",
    "The goal of this task is threefold:\n",
    "\n",
    "\n",
    "1. we want to reduce the noise in the original raw text by removing everything that does not bring information to the language model. everything that is not exactly text: html tags, math equations, urls, etc\n",
    "2. we want to prepare the corpus and make it ready for our language model by tokenizing the text. \n",
    "3. And finally, we want to remove rows with short or very long texts. As you will see, some of the entries are mostly made of large numerical tables. Entries that are too long will not be good reflection of the corpus. Entris that are too short will not bring relevant information to the language model either.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbiEgK7HS4L4"
   },
   "outputs": [],
   "source": [
    "# We only need the following librairies\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfW0QBdig_k7"
   },
   "source": [
    "Let's load the dataset and shuffle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWZx8tbrVrx8"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://alexip-ml.s3.amazonaws.com/stackexchange_812k.csv.gz', compression='gzip').sample(frac = 1, random_state = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgzBFBwWvG3q"
   },
   "outputs": [],
   "source": [
    "assert data.shape == (812132, 5), \"The dataset does not have the right dimensions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyWqejm3hUE-"
   },
   "source": [
    "And start by exploring the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "eiMeKSHbV7Lv",
    "outputId": "bb0bd4bd-6eb9-4c98-f1f7-1a47a8ecdff4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>291254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>601672.0</td>\n",
       "      <td>The condition makes the gradient unbiased. (it...</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>221284.0</td>\n",
       "      <td>Yes, that sounds fine to me.</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>327356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Consider gaussian variables belonging to a ...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>355055.0</td>\n",
       "      <td>Thanks S. Catterall. ^-^ Integrability: I knew...</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>433143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Feature with very few extreme values</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  ...  category\n",
       "0   291254  ...   comment\n",
       "1   115372  ...   comment\n",
       "2   327356  ...      post\n",
       "3   186923  ...   comment\n",
       "4   433143  ...     title\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bon5CFezh_v1"
   },
   "source": [
    "We have 3 types of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "oQK2WV0JWADk",
    "outputId": "98f300dd-f186-4abb-b45c-ddaac4b59b43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment    553076\n",
       "post       167304\n",
       "title       91752\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "vR-r4JgDiD5M",
    "outputId": "f08b65d9-05b0-499c-84a9-fafd45aae7e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Transform moments of a random variable to fit the moments of another\n",
      "--------------------\n",
      "Which method of implementing the Brown's linear exponential smoothing is correct?\n",
      "--------------------\n",
      "Test goodness of fit for geometric distribution\n"
     ]
    }
   ],
   "source": [
    "# example of titles \n",
    "for p in data[data.category == 'title'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iA47aMGKiSAk"
   },
   "source": [
    "We see that posts text have html tags and latex formatted equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "QagyZuf0iJqO",
    "outputId": "f7ecd5cf-8a19-4f5c-901e-e7a2ff847b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "<p>The following come to mind:</p>\n",
      "\n",
      "<ol>\n",
      "<li><p>Decision trees/random forests</p></li>\n",
      "<li><p>Support vector machines</p></li>\n",
      "<li><p>K Nearest Neighbor</p></li>\n",
      "</ol>\n",
      "\n",
      "<p>You will have to play around with and tune the models to see if they work, but you might get some good results. Given that Alzheimer's is pretty rare, it might take a lot of turning and domain knowledge (e.g. in the form of kernel functions for SVMs) to do better than just always predicting 0, but hopefully these models can get you started.</p>\n",
      "\n",
      "--------------------\n",
      "<p>I use neural network to do classification. But instead of outputing one label, I want to ouput four independent labels such as [-1,1,1,-1]. Each of them is either 1 or -1, indicating a classification for a specific part of my input(image). However, I have about 3/4 -1s and 1/4 1s in my dataset, so the network tends to predict -1 everywhere, I use a simple mean square error loss now. What can I do to avoid such situation? THX!</p>\n",
      "\n",
      "--------------------\n",
      "<p>Given that $X$ is a single observation from $f(x;\\theta)=(2\\theta x+1-\\theta)I_{[0,1]}(x)$ where $-1\\leq \\theta \\leq 1.$</p>\n",
      "\n",
      "<p>The objective is to test the hypothesis $H_0:\\theta\\leq0$ Vs $H_1:\\theta&gt;0$ where $H_0$ is rejected when $X&gt;\\frac{1}{2}$.</p>\n",
      "\n",
      "<p>I am a little confused about finding the power of the test. Power of this test is given as $1-\\beta$, where $\\beta=P(Accept\\:H_0\\:|\\:H_1)$. Hence, $1-\\beta=P(Reject\\:H_0\\:|\\:H_1)$ i.e. $P(X&gt;\\frac{1}{2})$ under $H_1$.</p>\n",
      "\n",
      "<p>Thus, $P(X&gt;\\frac{1}{2})=\\int_{\\frac{1}{2}}^{1}f(x;\\theta)dx=\\int_{\\frac{1}{2}}^{1}(2\\theta x+1-\\theta)dx=(\\theta x^2+x-\\theta x)|_{\\frac{1}{2}}^{1}=1-\\frac{1}{4}[2-\\theta]$.</p>\n",
      "\n",
      "<p>Obviously this is a function of $\\theta$ since under $H_1$, $\\theta &gt;0$ is unknown.</p>\n",
      "\n",
      "<p>Now in general in hypothesis testing what we do is, we fix the size of the test i.e we fix $P(Reject\\:H_0\\:|\\:H_0)$ and then maximize the power function $1-\\beta$ or equivalently minimize $\\beta$.</p>\n",
      "\n",
      "<p>So if we maximize the power function obtained above for $-1\\leq\\theta \\leq 1$, we obtain $(1-\\frac{1}{4})=\\frac{3}{4}$.</p>\n",
      "\n",
      "<p>Is that correct ?</p>\n",
      "\n",
      "<p>Also, can anyone provide a hint for finding a test for the given hypothesis that minimizes the largest of $\\alpha$ and $\\beta$. (For any general distribution).</p>\n",
      "\n",
      "<p>$H_0:\\theta = \\theta_1$ Vs $H_1:\\theta = \\theta_2$</p>\n",
      "\n",
      "<p>Are we supposed to follow that same technique i.e. fixing $\\alpha$ first and then handling $\\beta$ ? Or something else ?</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in data[data.category == 'post'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "No7lYssNiNCy",
    "outputId": "13568e3d-b64c-4d1f-904d-be913cbf2b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Thanks Arun, vote up for your answer. But for my example, I never predict anything to be positive `1`, I am confused about `fpr` and `tpr`, which the last element means when `fpr` is 1, `tpr` is also 1, any thoughts?\n",
      "--------------------\n",
      "I think you have to Go back a step and explain how you are getting inconsistent probabilities...maybe you need to fix that first. In what sense are they estimates..\n",
      "--------------------\n",
      "Elvis, I am not sure about the statement that $n=3k-1$ moments should imply a unique solution. For example, if $X$ is $\\mathcal{N}(\\mu,\\sigma)$ then $k=1$ is sufficient for any $n$. I would guess than $k>1$ would lead in this situation to many possible solutions.\n"
     ]
    }
   ],
   "source": [
    "# And here's a sample of comments\n",
    "for p in data[data.category == 'comment'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mk-vHb7LiZrf"
   },
   "source": [
    "# Clean up raw text\n",
    "We're going to remove the following elements:\n",
    "* html tags\n",
    "* line returns\n",
    "* urls\n",
    "* latex equations\n",
    "* numbers\n",
    "* mentions: @someone\n",
    "* digits\n",
    "* most of the punctuation\n",
    "* and extra spaces\n",
    "\n",
    "For that we will use a series of simple regex patterns and the following pandas dataframe pattern:\n",
    "\n",
    "```\n",
    "pattern = r\" some regex pattern\"\n",
    "df.text.apply(lambda t : re.sub(pattern,' ', t) )\n",
    "```\n",
    "\n",
    "Note that it's up to you to decide which elements should be removed or kept. This sequence of transformations can be modified. \n",
    "\n",
    "Not also that the regex patterns we use here are chosen for their simplicity. Feel free to use more precise patterns.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAr8qzoKiQjx"
   },
   "outputs": [],
   "source": [
    "# remove html tags\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"<[^>]*>\",' ', t) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DS_VHPC1jN5i"
   },
   "outputs": [],
   "source": [
    "# rm line returns\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"[\\r\\n]+\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyQeTFN9jPvK"
   },
   "outputs": [],
   "source": [
    "# rm urls\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"http\\S+\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TI1vZWUFjRCT"
   },
   "outputs": [],
   "source": [
    "# rm mentions\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"@\\S+\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "29nqngCpjST2"
   },
   "outputs": [],
   "source": [
    "# rm latex\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"\\$[^>]*\\$\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2ZBG7PkjTlQ"
   },
   "outputs": [],
   "source": [
    "# rm digits\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"\\d+\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEmEZVOwjVmy"
   },
   "outputs": [],
   "source": [
    "# rm some of the punctuation but keep ,.!? and -\n",
    "remove = '\"#$%&()*+/:;<=>@[\\\\]^_`{|}~”“'\n",
    "pattern = r\"[{}]\".format(remove)\n",
    "data['text'] = data.text.apply(lambda t : re.sub(pattern,' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhtXc_dPjX88"
   },
   "outputs": [],
   "source": [
    "# rm multiple spaces\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"\\s\\s+\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYJyjInajZQG"
   },
   "outputs": [],
   "source": [
    "# finally remove trailing spaces with strip()\n",
    "data['text'] = data.text.apply(lambda t : t.strip() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nY6RQnqSjbk3"
   },
   "source": [
    "Let's check out the resulting text for the different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "j7D5HfhXjZ8t",
    "outputId": "77b24bea-2d06-460e-dd13-ed9f66229658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Two small non-normally distributed samples and one simple question\n",
      "--------------------\n",
      "How to test the linearity between two non normal distributed variables\n",
      "--------------------\n",
      "What is a correct way to test these data for significance?\n"
     ]
    }
   ],
   "source": [
    "# titles should not be changed\n",
    "for p in data[data.category == 'title'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "_pyo6Bdhjl4J",
    "outputId": "4a185842-a6d7-4a3c-eaa0-51f86a5966f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Yes, that is all completely correct. Of course, if you need a value you don't know yet, you will need to feed in the already-forecasted values recursively. For instance, assume that you already know which you calculated at the previous step. In working with differenced series, it is often easier to work with differences and only convert them back to the original scale when you are all done. It makes the formulas simpler and the calculations less error-prone. Especially so once you start looking at second and higher differences.\n",
      "--------------------\n",
      "Is it is useful to use dummy variables even though it implies a big increase in the number of parameters? In my case, I have a training set of about observations, and I am using parameters. One of these is composed of different elements therefore whether I transform it in a dummy variable I'll have, in total, more than parameters. Is this transformation sensible?\n",
      "--------------------\n",
      "I find these classification systems extremely unhelpful and contradictory. For example neural networks is a form of supervised learning Calculus is used in differential geometry Probability theory can be formalized as a part of set theory and so on. There are no unambiguous branches of mathematics, and nor should there be of statistics.\n"
     ]
    }
   ],
   "source": [
    "# posts should have much less clutter\n",
    "for p in data[data.category == 'post'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "KKeBjpmsjxjZ",
    "outputId": "7301b185-7987-4503-8f63-20a1eb715962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "- because the hypothesis are exclusive and exhaustive\n",
      "--------------------\n",
      "Thank you, please have a look at Glen b's answer. Is this consistent with your answer?\n",
      "--------------------\n",
      "Follow-up then why is used as a non-informative distribution Jeffrey's Prior This is information, IMHO... the knowledge that and are more likely than . .\n"
     ]
    }
   ],
   "source": [
    "# comments should also be less noisy\n",
    "for p in data[data.category == 'comment'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4T1CQqWk6rP"
   },
   "source": [
    "# Tokenize\n",
    "\n",
    "Let's tokenize the text. \n",
    "This will allow us to count the number of tokens of each text and subsequently remove test that are too long or too short.\n",
    "You can use other librairies to tokenize the text (spacy for instance) or other tokenizer. Here we use the [WordPunctTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.WordPunctTokenizer) from NLTK.\n",
    "\n",
    "And we create a new columns called tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ge5LTc3j4Te"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "data['tokens'] = data.text.apply(lambda t : tokenizer.tokenize(t.lower())) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ll09OAjmGby"
   },
   "source": [
    "Let's now count the tokens in each piece of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3dp7L-Hl-AR"
   },
   "outputs": [],
   "source": [
    "data['n_tokens'] = data.tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "dzIE6rdWmPCo",
    "outputId": "40670ac8-d6c1-4909-e7fa-128630266d34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    812132.000000\n",
       "mean         60.074186\n",
       "std          99.416031\n",
       "min           0.000000\n",
       "25%          16.000000\n",
       "50%          35.000000\n",
       "75%          70.000000\n",
       "max       10874.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.n_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "PrwoRVQnmRa3",
    "outputId": "9b208aa6-0e3b-48d7-e15f-417e4b6536ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f24a74172b0>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAX2klEQVR4nO3df6xc5X3n8fe3OBAvDdhA98prW2tH\ntVJRUAhcgVGq6m7YGEOrmD9SBEL1XdaLVwupkg1Sa7Z/WIWNRFZL05hN3VjFxa7cEC9N1haBuF6H\n0Wr/MLFpKOaX6wsx62sBTmwwe0FN6u53/5jnkuFmnnvHF3vuzM37JY3mnO95zvOcZw6Zj+fMuZPI\nTCRJaueXZvoAJEm9y5CQJFUZEpKkKkNCklRlSEiSqubM9AGcaZdcckkuWbJkWvu+8847nH/++Wf2\ngHqI8+tvzq+/9fr8nn766R9n5q9MrM+6kFiyZAn79++f1r6NRoOhoaEze0A9xPn1N+fX33p9fhHx\naru6l5skSVWGhCSpypCQJFUZEpKkKkNCklRlSEiSqgwJSVKVISFJqjIkJElVU4ZERHwsIp5pebwd\nEV+IiIsiYndEHCrP80v7iIgNETESEc9GxJUtfQ2X9ociYrilflVEHCj7bIiIKPW2Y5wtB46eZMm6\n77Bk3XfO5jCS1DemDInMPJiZV2TmFcBVwLvAt4F1wJ7MXAbsKesANwDLymMtsBGab/jAeuAa4Gpg\nfcub/kbgjpb9VpZ6bQxJUhec7uWm64CXM/NVYBWwpdS3ADeV5VXA1mzaC8yLiAXA9cDuzDyRmW8C\nu4GVZdsFmbk3m/9fqlsn9NVuDElSF5zuD/zdAnyjLA9k5mtl+XVgoCwvBI607DNaapPVR9vUJxvj\nfSJiLc1PLQwMDNBoNE5rUuMG5sLdl58CmHYfvWxsbGxWzmuc8+tvzq83dRwSEXEu8BngnonbMjMj\nIs/kgZ3OGJm5CdgEMDg4mNP9pcUHt+3ggQPNl+TwbdPro5f1+q9QflDOr785v950OpebbgD+NjPf\nKOtvlEtFlOdjpX4UWNyy36JSm6y+qE19sjEkSV1wOiFxKz+71ASwExi/Q2kY2NFSX13ucloOnCyX\njHYBKyJifvnCegWwq2x7OyKWl7uaVk/oq90YkqQu6OhyU0ScD3wa+Pct5fuB7RGxBngVuLnUHwdu\nBEZo3gl1O0BmnoiI+4B9pd29mXmiLN8JPAzMBZ4oj8nGkCR1QUchkZnvABdPqB2nebfTxLYJ3FXp\nZzOwuU19P3BZm3rbMSRJ3eFfXEuSqgwJSVKVISFJqjIkJElVhoQkqcqQkCRVGRKSpCpDQpJUZUhI\nkqoMCUlSlSEhSaoyJCRJVYaEJKnKkJAkVRkSkqQqQ0KSVGVISJKqDAlJUpUhIUmqMiQkSVUdhURE\nzIuIRyPipYh4MSKujYiLImJ3RBwqz/NL24iIDRExEhHPRsSVLf0Ml/aHImK4pX5VRBwo+2yIiCj1\ntmNIkrqj008SXwW+m5m/BnwceBFYB+zJzGXAnrIOcAOwrDzWAhuh+YYPrAeuAa4G1re86W8E7mjZ\nb2Wp18aQJHXBlCERERcCvwk8BJCZP83Mt4BVwJbSbAtwU1leBWzNpr3AvIhYAFwP7M7ME5n5JrAb\nWFm2XZCZezMzga0T+mo3hiSpC+Z00GYp8CPgLyLi48DTwOeBgcx8rbR5HRgoywuBIy37j5baZPXR\nNnUmGeN9ImItzU8tDAwM0Gg0OpjWzxuYC3dffgpg2n30srGxsVk5r3HOr785v97USUjMAa4Efi8z\nn4qIrzLhsk9mZkTk2TjATsbIzE3AJoDBwcEcGhqa1hgPbtvBAweaL8nh26bXRy9rNBpM97XpB86v\nvzm/3tTJdxKjwGhmPlXWH6UZGm+US0WU52Nl+1Fgccv+i0ptsvqiNnUmGUOS1AVThkRmvg4ciYiP\nldJ1wAvATmD8DqVhYEdZ3gmsLnc5LQdOlktGu4AVETG/fGG9AthVtr0dEcvLXU2rJ/TVbgxJUhd0\ncrkJ4PeAbRFxLvAKcDvNgNkeEWuAV4GbS9vHgRuBEeDd0pbMPBER9wH7Srt7M/NEWb4TeBiYCzxR\nHgD3V8aQJHVBRyGRmc8Ag202XdembQJ3VfrZDGxuU98PXNamfrzdGJKk7vAvriVJVYaEJKnKkJAk\nVRkSkqQqQ0KSVGVISJKqDAlJUpUhIUmqMiQkSVWGhCSpypCQJFUZEpKkKkNCklRlSEiSqgwJSVKV\nISFJqjIkJElVhoQkqcqQkCRVGRKSpKqOQiIiDkfEgYh4JiL2l9pFEbE7Ig6V5/mlHhGxISJGIuLZ\niLiypZ/h0v5QRAy31K8q/Y+UfWOyMSRJ3XE6nyT+VWZekZmDZX0dsCczlwF7yjrADcCy8lgLbITm\nGz6wHrgGuBpY3/KmvxG4o2W/lVOMIUnqgg9yuWkVsKUsbwFuaqlvzaa9wLyIWABcD+zOzBOZ+Saw\nG1hZtl2QmXszM4GtE/pqN4YkqQvmdNgugb+JiAS+npmbgIHMfK1sfx0YKMsLgSMt+46W2mT10TZ1\nJhnjfSJiLc1PLQwMDNBoNDqc1vsNzIW7Lz8FMO0+etnY2NisnNc459ffnF9v6jQkfiMzj0bEPwd2\nR8RLrRszM0uAnDWTjVFCaxPA4OBgDg0NTWuMB7ft4IEDzZfk8G3T66OXNRoNpvva9APn19+cX2/q\n6HJTZh4tz8eAb9P8TuGNcqmI8nysND8KLG7ZfVGpTVZf1KbOJGNIkrpgypCIiPMj4iPjy8AK4Dlg\nJzB+h9IwsKMs7wRWl7uclgMnyyWjXcCKiJhfvrBeAewq296OiOXlrqbVE/pqN4YkqQs6udw0AHy7\n3JU6B/irzPxuROwDtkfEGuBV4ObS/nHgRmAEeBe4HSAzT0TEfcC+0u7ezDxRlu8EHgbmAk+UB8D9\nlTEkSV0wZUhk5ivAx9vUjwPXtakncFelr83A5jb1/cBlnY4hSeoO/+JaklRlSEiSqgwJSVKVISFJ\nqjIkJElVhoQkqcqQkCRVGRKSpCpDQpJUZUhIkqoMCUlSlSEhSaoyJCRJVYaEJKnKkJAkVRkSkqQq\nQ0KSVGVISJKqDAlJUpUhIUmq6jgkIuKciPhBRDxW1pdGxFMRMRIR34yIc0v9vLI+UrYvaenjnlI/\nGBHXt9RXltpIRKxrqbcdQ5LUHafzSeLzwIst618GvpKZvwq8Cawp9TXAm6X+ldKOiLgUuAX4dWAl\n8KcleM4BvgbcAFwK3FraTjaGJKkLOgqJiFgE/Bbw52U9gE8Bj5YmW4CbyvKqsk7Zfl1pvwp4JDN/\nkpk/BEaAq8tjJDNfycyfAo8Aq6YYQ5LUBXM6bPcnwO8DHynrFwNvZeapsj4KLCzLC4EjAJl5KiJO\nlvYLgb0tfbbuc2RC/ZopxnifiFgLrAUYGBig0Wh0OK33G5gLd1/eHG66ffSysbGxWTmvcc6vvzm/\n3jRlSETEbwPHMvPpiBg6+4d0+jJzE7AJYHBwMIeGhqbVz4PbdvDAgeZLcvi26fXRyxqNBtN9bfqB\n8+tvzq83dfJJ4pPAZyLiRuDDwAXAV4F5ETGn/Et/EXC0tD8KLAZGI2IOcCFwvKU+rnWfdvXjk4wh\nSeqCKb+TyMx7MnNRZi6h+cXz9zLzNuBJ4LOl2TCwoyzvLOuU7d/LzCz1W8rdT0uBZcD3gX3AsnIn\n07lljJ1ln9oYkqQu+CB/J/EHwBcjYoTm9wcPlfpDwMWl/kVgHUBmPg9sB14AvgvclZn/VD4lfA7Y\nRfPuqe2l7WRjSJK6oNMvrgHIzAbQKMuv0LwzaWKbfwB+p7L/l4Avtak/Djzept52DElSd/gX15Kk\nKkNCklRlSEiSqgwJSVKVISFJqjIkJElVhoQkqcqQkCRVGRKSpCpDQpJUZUhIkqoMCUlSlSEhSaoy\nJCRJVYaEJKnKkJAkVRkSkqQqQ0KSVGVISJKqDAlJUtWUIRERH46I70fE30XE8xHxR6W+NCKeioiR\niPhmRJxb6ueV9ZGyfUlLX/eU+sGIuL6lvrLURiJiXUu97RiSpO7o5JPET4BPZebHgSuAlRGxHPgy\n8JXM/FXgTWBNab8GeLPUv1LaERGXArcAvw6sBP40Is6JiHOArwE3AJcCt5a2TDKGJKkLpgyJbBor\nqx8qjwQ+BTxa6luAm8ryqrJO2X5dRESpP5KZP8nMHwIjwNXlMZKZr2TmT4FHgFVln9oYkqQu6Og7\nifIv/meAY8Bu4GXgrcw8VZqMAgvL8kLgCEDZfhK4uLU+YZ9a/eJJxpAkdcGcThpl5j8BV0TEPODb\nwK+d1aM6TRGxFlgLMDAwQKPRmFY/A3Ph7submTTdPnrZ2NjYrJzXOOfX35xfb+ooJMZl5lsR8SRw\nLTAvIuaUf+kvAo6WZkeBxcBoRMwBLgSOt9THte7Trn58kjEmHtcmYBPA4OBgDg0Nnc603vPgth08\ncKD5khy+bXp99LJGo8F0X5t+4Pz6m/PrTZ3c3fQr5RMEETEX+DTwIvAk8NnSbBjYUZZ3lnXK9u9l\nZpb6LeXup6XAMuD7wD5gWbmT6VyaX27vLPvUxpAkdUEnnyQWAFvKXUi/BGzPzMci4gXgkYj4z8AP\ngIdK+4eAv4yIEeAEzTd9MvP5iNgOvACcAu4ql7GIiM8Bu4BzgM2Z+Xzp6w8qY0iSumDKkMjMZ4FP\ntKm/QvPOpIn1fwB+p9LXl4Avtak/Djze6RiSpO7wL64lSVWGhCSpypCQJFUZEpKkKkNCklRlSEiS\nqgwJSVKVISFJqjIkJElVhoQkqcqQkCRVGRKSpCpDQpJUZUhIkqoMCUlSlSEhSaoyJCRJVYaEJKnK\nkJAkVRkSkqSqKUMiIhZHxJMR8UJEPB8Rny/1iyJid0QcKs/zSz0iYkNEjETEsxFxZUtfw6X9oYgY\nbqlfFREHyj4bIiImG0OS1B2dfJI4BdydmZcCy4G7IuJSYB2wJzOXAXvKOsANwLLyWAtshOYbPrAe\nuAa4Gljf8qa/EbijZb+VpV4bQ5LUBVOGRGa+lpl/W5b/L/AisBBYBWwpzbYAN5XlVcDWbNoLzIuI\nBcD1wO7MPJGZbwK7gZVl2wWZuTczE9g6oa92Y0iSuuC0vpOIiCXAJ4CngIHMfK1seh0YKMsLgSMt\nu42W2mT10TZ1JhlDktQFczptGBG/DPw18IXMfLt8bQBAZmZE5Fk4vo7GiIi1NC9tMTAwQKPRmNYY\nA3Ph7stPAUy7j142NjY2K+c1zvn1N+fXmzoKiYj4EM2A2JaZ3yrlNyJiQWa+Vi4ZHSv1o8Dilt0X\nldpRYGhCvVHqi9q0n2yM98nMTcAmgMHBwRwaGmrXbEoPbtvBAweaL8nh26bXRy9rNBpM97XpB86v\nvzm/3tTJ3U0BPAS8mJl/3LJpJzB+h9IwsKOlvrrc5bQcOFkuGe0CVkTE/PKF9QpgV9n2dkQsL2Ot\nntBXuzEkSV3QySeJTwK/CxyIiGdK7T8B9wPbI2IN8Cpwc9n2OHAjMAK8C9wOkJknIuI+YF9pd29m\nnijLdwIPA3OBJ8qDScaQJHXBlCGRmf8biMrm69q0T+CuSl+bgc1t6vuBy9rUj7cbQ5LUHf7FtSSp\nypCQJFUZEpKkKkNCklRlSEiSqgwJSVKVISFJqjIkJElVhoQkqcqQkCRVGRKSpCpDQpJUZUhIkqoM\nCUlSlSEhSaoyJCRJVYaEJKnKkJAkVRkSkqQqQ0KSVGVISJKqpgyJiNgcEcci4rmW2kURsTsiDpXn\n+aUeEbEhIkYi4tmIuLJln+HS/lBEDLfUr4qIA2WfDRERk40hSeqeTj5JPAysnFBbB+zJzGXAnrIO\ncAOwrDzWAhuh+YYPrAeuAa4G1re86W8E7mjZb+UUY0iSumTKkMjM/wWcmFBeBWwpy1uAm1rqW7Np\nLzAvIhYA1wO7M/NEZr4J7AZWlm0XZObezExg64S+2o0hSeqSOdPcbyAzXyvLrwMDZXkhcKSl3Wip\nTVYfbVOfbIyfExFraX5yYWBggEajcZrTKQPOhbsvPwUw7T562djY2Kyc1zjn19+cX2+abki8JzMz\nIvJMHMx0x8jMTcAmgMHBwRwaGprWOA9u28EDB5ovyeHbptdHL2s0Gkz3tekHzq+/Ob/eNN27m94o\nl4ooz8dK/SiwuKXdolKbrL6oTX2yMSRJXTLdkNgJjN+hNAzsaKmvLnc5LQdOlktGu4AVETG/fGG9\nAthVtr0dEcvLXU2rJ/TVbgxJUpdMebkpIr4BDAGXRMQozbuU7ge2R8Qa4FXg5tL8ceBGYAR4F7gd\nIDNPRMR9wL7S7t7MHP8y/E6ad1DNBZ4oDyYZQ5LUJVOGRGbeWtl0XZu2CdxV6WczsLlNfT9wWZv6\n8XZjSJK6x7+4liRVGRKSpCpDQpJUZUhIkqo+8B/TzVZL1n3nveXD9//WDB6JJM0cP0lIkqoMCUlS\nlSEhSaoyJCRJVYaEJKnKkJAkVRkSkqQqQ0KSVGVISJKqDAlJUpUhIUmqMiQkSVX+wF8H/LE/Sb+o\n/CQhSaoyJCRJVT0fEhGxMiIORsRIRKyb6eORpF8kPf2dREScA3wN+DQwCuyLiJ2Z+cJMHZPfT0j6\nRdLTIQFcDYxk5isAEfEIsAqYsZBo1RoYrQwPSbNFr4fEQuBIy/oocM3ERhGxFlhbVsci4uA0x7sE\n+PE09/3Z8Xz5g/Zw1pyR+fUw59ffnN/M+pftir0eEh3JzE3Apg/aT0Tsz8zBM3BIPcn59Tfn19/6\ndX69/sX1UWBxy/qiUpMkdUGvh8Q+YFlELI2Ic4FbgJ0zfEyS9Aujpy83ZeapiPgcsAs4B9icmc+f\nxSE/8CWrHuf8+pvz6299Ob/IzJk+BklSj+r1y02SpBlkSEiSqgyJoh9//iMiFkfEkxHxQkQ8HxGf\nL/WLImJ3RBwqz/NLPSJiQ5njsxFxZUtfw6X9oYgYnqk5tRMR50TEDyLisbK+NCKeKvP4ZrmpgYg4\nr6yPlO1LWvq4p9QPRsT1MzOTnxcR8yLi0Yh4KSJejIhrZ9P5i4j/WP7bfC4ivhERH+7n8xcRmyPi\nWEQ811I7Y+crIq6KiANlnw0REd2dYRuZ+Qv/oPml+MvAR4Fzgb8DLp3p4+rguBcAV5bljwB/D1wK\n/BdgXamvA75clm8EngACWA48VeoXAa+U5/llef5Mz69lnl8E/gp4rKxvB24py38G/IeyfCfwZ2X5\nFuCbZfnSck7PA5aWc33OTM+rHNsW4N+V5XOBebPl/NH8Y9gfAnNbztu/6efzB/wmcCXwXEvtjJ0v\n4PulbZR9b5jx8zjTB9ALD+BaYFfL+j3APTN9XNOYxw6av3N1EFhQaguAg2X568CtLe0Plu23Al9v\nqb+v3QzPaRGwB/gU8Fj5H8+PgTkTzx3Nu+CuLctzSruYeD5b283w3C4sb6IxoT4rzh8/+8WEi8r5\neAy4vt/PH7BkQkickfNVtr3UUn9fu5l6eLmpqd3PfyycoWOZlvLR/BPAU8BAZr5WNr0ODJTl2jx7\nef5/Avw+8P/K+sXAW5l5qqy3Hut78yjbT5b2vTq/pcCPgL8ol9P+PCLOZ5acv8w8CvxX4P8Ar9E8\nH08ze87fuDN1vhaW5Yn1GWVIzAIR8cvAXwNfyMy3W7dl858kfXmfc0T8NnAsM5+e6WM5S+bQvHSx\nMTM/AbxD83LFe/r8/M2n+YOcS4F/AZwPrJzRgzrL+vl81RgSTX378x8R8SGaAbEtM79Vym9ExIKy\nfQFwrNRr8+zV+X8S+ExEHAYeoXnJ6avAvIgY/0PQ1mN9bx5l+4XAcXp3fqPAaGY+VdYfpRkas+X8\n/Wvgh5n5o8z8R+BbNM/pbDl/487U+TpalifWZ5Qh0dSXP/9R7nx4CHgxM/+4ZdNOYPyOiWGa31WM\n11eXuy6WAyfLx+RdwIqImF/+9bei1GZUZt6TmYsycwnNc/K9zLwNeBL4bGk2cX7j8/5saZ+lfku5\ne2YpsIzmF4QzKjNfB45ExMdK6TqaP4M/K84fzctMyyPin5X/VsfnNyvOX4szcr7KtrcjYnl5vVa3\n9DVzZvpLkV550LwT4e9p3jnxhzN9PB0e82/Q/Gj7LPBMedxI8zruHuAQ8D+Bi0r7oPl/4vQycAAY\nbOnr3wIj5XH7TM+tzVyH+NndTR+l+SYxAvx34LxS/3BZHynbP9qy/x+WeR+kB+4YaTmuK4D95Rz+\nD5p3u8ya8wf8EfAS8BzwlzTvUOrb8wd8g+b3K/9I85PgmjN5voDB8lq9DPw3JtzUMBMPf5ZDklTl\n5SZJUpUhIUmqMiQkSVWGhCSpypCQJFUZEpKkKkNCklT1/wFMqIrRiW5FigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.n_tokens.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJGNrR5wmV5r"
   },
   "source": [
    "We see that we have some extremely long texts. Let's look at the largest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "SgsxoSdWmUe3",
    "outputId": "6a72f8d1-d2ac-45b7-b1c9-0e669b025187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My sample includes subjects, of which belong to group L , while the other to group L please see data below . I used GLM for a binary outcome to test for group differences in background variables - summary pre lt - glm L g a m p e, family binomial logit , data df ...yielding significant differences for of them g, a, m, p, and e . So I modeled these background variables as covariates when testing for an association between my predictor, chr and my outcome rsk , in each one of the groups L , L , again using GLM for binary outcome summary fit lt - glm rsk chr g a m p e, family binomial logit , data df which df L , The results showed that a significant association does exist for L but not for L . I would appreciate your help in how to test whether significance non-significance can be attributed to the group condition? . Or in other words, is it true that for subjects L , a significant correlation is evident, while for L ' it's absent. Thanks for responders! Uri structure list L c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , g c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , a c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , m c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , p c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , e c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , chr c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , rsk c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , row.names c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , class data.frame\n"
     ]
    }
   ],
   "source": [
    "# this one has a very long series of \"L,\"\n",
    "print(data[data.n_tokens > 10000].text.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nK4wiFJamvac"
   },
   "source": [
    "We can see that most of the longest texts are composed of tables with limited semantic value. \n",
    "We will remove rows that have more than an arbitrary number of tokens (let's say 5000) as well as rows that have too few tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cw8QXJ9CmjTo",
    "outputId": "5466e0dd-77c2-45ea-bd71-796063abbbe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789649, 7)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[(data.n_tokens > 4) & (data.n_tokens < 5000)].reset_index(drop = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "P2IOaPeKsDHO",
    "outputId": "7a2883b6-6a39-48ac-df35-fec8290ebe4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment    540587\n",
       "post       165377\n",
       "title       83685\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fe4WDsdFnQ29"
   },
   "source": [
    "# Export data\n",
    "We could export the dataframe as such using a pickle file format. \n",
    "\n",
    "However if we want to keep the original csv format it's going to be easier if we transform the list of tokens into a space separated string.\n",
    "\n",
    "On retrieval we will only have to split the string to get back the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "vgqfut5snKj4",
    "outputId": "91673f01-cb41-45ee-f755-60a15efb3ade"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the condition makes the gradient unbiased . it...\n",
       "1                       yes , that sounds fine to me .\n",
       "2    consider gaussian variables belonging to a gau...\n",
       "3    thanks s . catterall . - integrability i knew ...\n",
       "4                 feature with very few extreme values\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data.tokens.apply(lambda tk : ' '.join(tk))\n",
    "data.tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wfr2MyKoAtn"
   },
   "source": [
    "And finally let's export the dataframe into a csv file.\n",
    "We will use that csv file as the new cleaned up and filtered out dataset to build our language model in task 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtMO9S-Jn9Nf"
   },
   "outputs": [],
   "source": [
    "data.to_csv('stackexchange_812k.tokenized.csv', quoting = csv.QUOTE_ALL, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6VZYBr8oYEY"
   },
   "source": [
    "# Conclusion\n",
    "Removing or adding steps to this first text processing task will allow us to test different approaches in our language model building process.\n",
    "\n",
    "For instance we can decide not to remove the latex formatted mathematical equation and see if the language model is able to create grammatically valid equations. \n",
    "\n",
    "We could also implement a step to handle contractions (i'm, let's, ...) and see if that improves the quality of the generated text\n",
    "\n",
    "Finally we could also decide to work on the vocabulary and filter out typos or non-English unknown words using named entity recognition to tag specific tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgEZZVcjoU6Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LPLMDL Task 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
